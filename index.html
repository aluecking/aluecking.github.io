<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Home of Andy Lücking</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Andy Lücking</strong></a>
									<ul class="icons">
										<li><a href="https://orcid.org/0000-0002-5070-2233" target="_blank" class="icon brands fa-orcid"><span class="label">ORCID</span></a></li>
										<li><a href="https://www.researchgate.net/profile/Andy-Luecking" target="_blank" class="icon brands fa-researchgate"><span class="label">ResearchGate</span></a></li>
									</ul>
								</header>

							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h2>Welcome</h2>
										</header>
										<p>I'm a linguistic whose research contributes to a linguistic theory of human communication, that is, face-to-face interaction beyond single sentences. This involves the adaptation of dynamic dialogue semantics, the development of multimodal grammar extensions, occasionally the revision of traditional linguistic theories (e.g., quantification or pointing), the use of corpora and computational methods, and taking an overarching cognitive perspective. Andy Lücking received a PhD in linguistics (Dr. phil.) in 2011 at Bielefeld University on iconicity and iconic gestures. He defended his habilitation in 2022 on <q>Aspects of multimodal communication</q> at the Laboratoire de Linguistique Formelle (LLF) at the Université Paris Cité.</p>
										<p>Currently I'm a Privatdozent at <a href="https://www.uni-frankfurt.de/" target="_blank">Goethe University Frankfurt</a>, <a href="https://www.texttechnologylab.org/" target="_blank">Text Technology Lab</a> and a postdoc reseacher at <a href="https://u-paris.fr/en/" target="_blank">Université Paris Cité</a>, <a href="http://www.llf.cnrs.fr/" target="_blank">Laboratoire de Linguistique Formelle</a> (LLF).</p>
									</div>
									<span class="image object">
										<img src="images/AL-2019.jpg" alt="" style="width: 40%;height: 95%;overflow: hidden">
									</span>
								</section>




							<!-- Section -->
								<section>
									<header class="major">
										<h2 id="publications">Publications</h2>
									<p>Below a selection of works I like most. For the full list of publications please see the <i>Publications</i> menu at <a href="https://www.texttechnologylab.org/team/andy-luecking/" target="_blank">the TTLab page</a>.</p>
									</header>
																		
									<div class="posts">
										<article>
<!--											<a href="#" class="image"><img src="images/vr-move.jpg" alt="" /></a>
											<h3>GeMDiS</h3>-->
											<p>Andy Lücking and Jonathan Ginzburg. <strong>Leading voices: Dialogue semantics, cognitive science, and the polyphonic structure of multimodal interaction</strong>. Language and Cognition , Volume 15 , Issue 1 , January 2023 , pp. 148&ndash;172, DOI: <a href="https://doi.org/10.1017/langcog.2022.30" target="_blank">10.1017/langcog.2022.30</a>. (<a href="https://www.researchgate.net/publication/365654991_Leading_voices_Dialogue_semantics_cognitive_science_and_the_polyphonic_structure_of_multimodal_interaction" target="_blank">Preprint</a>)</p>
											<p>Multimodal communication from a dialogue semantic point of view, formulates the <em>multimodal serialization hypothesis</em>.</p>
										</article>
										
										<article><p>Andy Lücking and Jonathan Ginzburg. <strong>Referential transparency as the proper treatment of quantification</strong>. In: Semantics and Pragmatics 15, 4 (2022). DOI: <a href="https://doi.org/10.3765/sp.15.4" target="_blank">10.3765/sp.15.4</a>. (<a href="https://semprag.org/index.php/sp/article/view/sp.15.4" target="_blank">Early access</a>)</p>
										<p>A dialogue- and gesture-friendly theory of quantification.</p>
										</article>
										
										<article><p>Andy Lücking. <strong>Gesture</strong>. In: Head Driven Phrase Structure Grammar: The handbook. Ed. by Stefan Müller, Anne Abeillé, Robert D. Borsley and Jean-Pierre
Koenig. Empirically Oriented Theoretical Morphology and Syntax 9. Berlin: Language Science Press, 2021. Chap. 27, pp. 1201&ndash;1250. DOI: <a href="https://doi.org/10.5281/zenodo.5543318" target="_blank">10.5281/zenodo.5543318</a>. URL: <a href="https://langsci-press.org/catalog/book/259" target="_blank">https://langsci-press.org/catalog/book/259</a>.</p>	
									</article>

									<article><p>Andy Lücking, Jonathan Ginzburg and Robin Cooper. <strong>Grammar in dialogue</strong>. In: Head Driven Phrase Structure Grammar: The handbook. Ed. by Stefan Müller, Anne Abeillé, Robert D. Borsley and Jean-Pierre Koenig. Empirically Oriented Theoretical Morphology and Syntax 9. Berlin: Language Science Press, 2021. Chap. 26, pp. 1155&ndash;1199. DOI: <a href="https://doi.org/10.5281/zenodo.5543318" target="_blank">10.5281/zenodo.5543318</a>. URL: <a href="https://langsci-press.org/catalog/book/259" target="_blank">https://langsci-press.org/catalog/book/259</a>.</p></article>
									
									<article><p>Jonathan Ginzburg and Andy Lücking. <strong>I thought pointing is rude: A dialogue-semantic analysis of pointing at the addressee</strong>. In: Proceedings of Sinn und Bedeutung 25. Ed. by Patrick Grosz, Luisa Martí, Hazel Pearson, Yasutada Sudo and Sarah Zobel. SuB 25. Special Session: Gestures and Natural Language Semantics. University College London (Online), 2021, pp. 276&ndash;291. URL: <a href="https:
//ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/937" target="_blank">https:
//ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/937</a>.</p>
									<p>Analysing discourse pointing.</p>
									</article>

									<article><p>Jonathan Ginzburg and Andy Lücking. <strong>On Laughter and Forgetting and Reconversing: A neurologically-inspired model of conversational context</strong>. In: Proceedings of the 24th Workshop on the Semantics and Pragmatics of Dialogue. SemDial/WatchDial. Brandeis University, Waltham, New Jersey (Online), 2020. URL: <a href="http://semdial.org/anthology/Z20-Ginzburg_semdial_0008.pdf" target="_blank">http://semdial.org/anthology/Z20-Ginzburg_semdial_0008.pdf</a>.</p>
									<p>Context in dialogue semantics as memory structures.</p>
									</article>
									
									<article><p>Andy Lücking. <strong>Witness-loaded and Witness-free Demonstratives</strong>. In: Atypical Demonstratives. Syntax, Semantics and Pragmatics. Ed. by Marco Coniglio, Andrew Murphy, Eva Schlachter and Tonjes Veenstra. Linguistische Arbeiten 568. Berlin und Boston: De Gruyter, 2018, pp. 255&ndash;284. (<a href="https://www.researchgate.net/publication/303667514_Witness-loaded_and_Witness-free_Demonstratives" target="_blank">Preprint</a>)</p>
									<p>Demonstration acts as search instructions.</p></article>
									
									<article><p>Andy Lücking, Thies Pfeiffer and Hannes Rieser. <strong>Pointing and Reference Reconsidered</strong>. In: Journal of Pragmatics 77 (2015), pp. 56&ndash;79. DOI: <a href="https:doi.org/10.1016/j.pragma.2014.12.013" target="_blank">10.1016/j.pragma.2014.12.013</a>.</p>
									<p>There is no such thing as direct reference...</p></article>
																		
									<article><p>Andy Lücking. <strong>Ikonische Gesten. Grundzüge einer linguistischen Theorie</strong>. Berlin and Boston: De Gruyter, 2013.</p>
									<p>Grounding iconic gesture meaning in semantic model by means of exemplification. Sets up a phonetic-kinematic gesture interface in grammar.</p></article>
									
									<article><p>Andy Lücking, Sebastian Ptock and Kirsten Bergmann. <strong>Assessing Agreement on Segmentations by Means of Staccato, the Segmentation Agreement Calculator
according to Thomann</strong>. In: Gesture and Sign Language in Human-Computer Interaction and Embodied Communication. 9th International Gesture Workshop, GW 2011, Athens, Greece, May 2011, Revised Selected Papers. Ed. by Eleni Efthimiou, Georgios Kouroupetroglou and Stavroula-Evita Fotina. Lecture Notes in Artificial Intelligence 7206. Berlin und Heidelberg: Springer, 2012, pp. 129&ndash;138. DOI: <a href="https://doi.org/10.1007/978-3-642-34182-3_12" target="_blank">10.1007/978-3-642-34182-3_12</a>.</p><p>Staccato is installed in the video annotation tool <a href="https://archive.mpi.nl/tla/elan" target="_blank">ELAN</a>.</p></article>

									<article><p>Andy Lücking, Kirsten Bergmann, Florian Hahn, Stefan Kopp and Hannes Rieser. <strong>The Bielefeld Speech and Gesture Alignment Corpus (SaGA)</strong>. In: Multimodal
Corpora: Advances in Capturing, Coding and Analyzing Multimodality. LREC 2010. 7th International Conference for Language Resources und Evaluation. Malta, 2010, pp. 92&ndash;98. DOI: <a href="https://doi.org/10.13140/2.1.4216.1922" target="_blank">10.13140/2.1.4216.1922</a>.</p></article>

									<article><p>Andy Lücking, Alexander Mehler and Peter Menke. <strong>Taking Fingerprints of Speech-and-Gesture Ensembles: Approaching Empirical Evidence of Intrapersonal Alignmnent in Multimodal Communication</strong>. In: Proceedings of the 12th Workshop on the Semantics and Pragmatics of Dialogue. LonDial'08. King’s College London, 2008, pp. 157&ndash;164. URL: <a href="http://semdial.org/anthology/Z08-Lucking_semdial_0026.pdf" target="_blank">http://semdial.org/anthology/Z08-Lucking_semdial_0026.pdf</a>.</p></article>
									</div>
								</section>
		
								
								

							<!-- Section -->
								<section>
									<header class="major">
										<h2 id="courses">Workshops</h2>
									</header>
									<div class="features">
										<article>
											<span class="icon solid fa-sign-language"></span>
											<div class="content">
												<h3>ESSLLI 2022</h3>
												<p>Workshop <a href="https://aluecking.github.io/ESSLLI2022/" target="_blank">Multimodal Interaction in Dialogue and its Meaning</a> at ESSLLI 2022, Galway, Ireland, August 15--19 (with <a href="https://sites.google.com/site/jonathanginzburgswebsite/" target="_blank">Jonathan Ginzburg</a>).</p>
											</div>
										</article>
										<article>
											<span class="icon solid fa-brain"></span>
											<div class="content">
												<h3>NASSLLI 2022</h3>
												<p>Workshop <a href="https://aluecking.github.io/NASSLLI2022/" target="_blank">Dialogue across the lifespan</a> at NASSLLI 2022, USC, LA (with <a href="https://sites.google.com/site/jonathanginzburgswebsite/" target="_blank">Jonathan Ginzburg</a>).</p>
											</div>
										</article>
										<article>
											<span class="icon solid fa-university"></span>
											<div class="content">
												<h3>Dublin 2004</h3>
												<p><i>A first session with the LKB system</i> at Trinity College, Dublin (with <a href="https://www.researchgate.net/profile/Hannes-Rieser" target="_blank">Hannes Rieser</a>).</p>
											</div>
										</article>
									</div>
								</section>
								
								
								
								<!-- Section -->
								<section>
									<header class="major">
										<h2 id="projects">Projects</h2>
									</header>
									<div class="posts">
										<article>
											<a href="#" class="image"><img src="images/vr-move.jpg" alt="" /></a>
											<p style="font-size:9px">CC BY-SA 4.0, digmedia.lucdh.nl</p>
											<h3>GeMDiS</h3>
											<p><strong>Virtual Reality Sustained Multimodal Distributional Semantics for Gestures in Dialogue (GeMDiS)</strong>.</p> 
											<p>Due to a lack of apt corpora, "multimodal linguistics" and dialogue theory cannot participate in established distributional methods of corpus linguistics and computational semantics. The main reason for this is the difficulty of collecting multimodal data in an appropriate way and at an appropriate scale. Using the latest VR-based recording methods, the GeMDiS project aims to close this data gap and to investigate visual communication by means of machine-based methods and innovative use of neuronal and active learning for small data using the systematic reference dimensions of associativity and contiguity of the features of visual and non-visual communicative signs.</p>
											<p>GeMDiS is part of the DFG prority programme <a href="https://vicom.info/" target="_blank">Visual Communication</a>.</p>
											<ul class="actions">
												<li><a href="https://vicom.info/virtual-reality-sustained-multimodal-distributional-semantics-for-gestures-in-dialogue-gemdis/" target="_blank" class="button">More</a></li>
											</ul>
										</article>
									</div>
								</section>

						</div>
					</div>
					
					
					
				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
							<!--	<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>
							-->
							
							
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="#publications">Publications</a></li>
										<li><a href="#courses">Workshops</a></li>
										<li><a href="#projects">Projects</a></li>
									</ul>
								</nav>



							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="mailto:luecking@em.uni-frankfurt.de">luecking@em.uni-frankfurt.de</a></li>
										<li class="icon solid fa-home">Goethe University Frankfurt<br />
										Text Technology Lab<br />
										Robert-Mayer-Straße 10<br />
										60325 Frankfurt am Main, Germany</li>
										<li class="icon solid fa-home">Université Paris Cité<br />
										Laboratoire de Linguistique Formelle<br />
										5 Rue Thomas Mann<br /> 
										75013 Paris, France</li>
									</ul>
								</section>



							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Andy Lücking. All rights reserved. GPL 3.0. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>
						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>